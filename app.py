import streamlit as st
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import os

# --- ì„¤ì • ë° ë³´ì•ˆ ---
ST_API_KEY = st.secrets["GEMINI_API_KEY"]
MODEL_NAME = "gemini-2.0-flash"  # ìµœì‹  ì•ˆì •í™” ëª¨ë¸ ì‚¬ìš©

st.set_page_config(page_title="PDF ìš”ì • ì±—ë´‡", layout="centered")

# --- ì¹´ì¹´ì˜¤í†¡ ìŠ¤íƒ€ì¼ CSS ---
st.markdown("""
<style>
    .stApp { background-color: #abc1d1; }
    .chat-message {
        padding: 10px; border-radius: 10px; margin-bottom: 10px;
        display: flex; flex-direction: column;
    }
    .chat-message.user {
        background-color: #fee500; align-self: flex-end;
        color: #3c3e3f; margin-left: 20%;
    }
    .chat-message.bot {
        background-color: #ffffff; align-self: flex-start;
        color: #3c3e3f; margin-right: 20%;
    }
    .chat-bubble { padding: 8px 12px; border-radius: 15px; font-size: 14px; }
</style>
""", unsafe_allow_html=True)

# --- PDF ì²˜ë¦¬ ë° RAG ì—”ì§„ êµ¬ì¶• ---
def setup_rag(uploaded_file):
    with open("temp.pdf", "wb") as f:
        f.write(uploaded_file.getbuffer())
    
    loader = PyPDFLoader("temp.pdf")
    documents = loader.load()
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    texts = text_splitter.split_documents(documents)
    
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=ST_API_KEY)
    vector_db = FAISS.from_documents(texts, embeddings)
    
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • (ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•˜ë„ë¡ ì œì•½)
    template = """ë‹¹ì‹ ì€ ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.
    ë¬¸ì„œì˜ ë‚´ìš©ì— ì—†ëŠ” ì§ˆë¬¸ì´ê±°ë‚˜ í™•ì‹¤í•˜ì§€ ì•Šì€ ê²½ìš°, "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ë‚´ìš©ì€ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
    ë§íˆ¬ëŠ” ì¹œì ˆí•˜ê³  ê·€ì—½ê²Œ í•˜ì„¸ìš”.

    Context: {context}
    Question: {question}
    Answer:"""
    
    QA_PROMPT = PromptTemplate(template=template, input_variables=["context", "question"])
    
    llm = ChatGoogleGenerativeAI(model=MODEL_NAME, google_api_key=ST_API_KEY, temperature=0.1)
    
    return RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vector_db.as_retriever(),
        chain_type_kwargs={"prompt": QA_PROMPT}
    )

# --- UI ì„¸ì…˜ ê´€ë¦¬ ---
if "messages" not in st.session_state:
    st.session_state.messages = []
if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None

# --- ì‚¬ì´ë“œë°” ë° íŒŒì¼ ì—…ë¡œë“œ ---
with st.sidebar:
    st.title("ğŸ’› PDF ì±„íŒ…ë°©")
    uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”", type="pdf")
    if uploaded_file:
        with st.spinner("ë¬¸ì„œë¥¼ ì½ê³  ìˆìŠµë‹ˆë‹¤..."):
            st.session_state.rag_chain = setup_rag(uploaded_file)
            st.success("ì¤€ë¹„ ì™„ë£Œ!")

# --- ë©”ì¸ ì±„íŒ… í™”ë©´ ---
st.title("ğŸ’¬ PDF ìš”ì •")

# ì±„íŒ… ë‚´ì—­ í‘œì‹œ
for message in st.session_state.messages:
    role_class = "user" if message["role"] == "user" else "bot"
    st.markdown(f"""
    <div class="chat-message {role_class}">
        <div class="chat-bubble">{message["content"]}</div>
    </div>
    """, unsafe_allow_html=True)

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ê¶ê¸ˆí•œ ê²ƒì„ ë¬¼ì–´ë³´ì„¸ìš”!"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # ë‹µë³€ ìƒì„±
    if st.session_state.rag_chain:
        response = st.session_state.rag_chain.invoke(prompt)
        answer = response["result"]
    else:
        answer = "ë¨¼ì € ì™¼ìª½ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”! ğŸ“"

    st.session_state.messages.append({"role": "bot", "content": answer})
    st.rerun()